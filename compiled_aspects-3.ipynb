{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X5oSeZadj3AZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "14da91d3-6ed6-4846-c27d-b7e324819a7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n4.1.3 LangChain/LangGraph Orchestration\\nWe will use LangChain and LangGraph (from Modules 4, 9, 10) to manage the workflow.\\nLangGraph will handle the conversation flow by checking question safety, retrieving\\nrelevant passages, generating a response, validating response safety, and sending it to the\\nuser. If something fails the safety checks, the system can try again or use a backup\\nresponse.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "4.1.3 LangChain/LangGraph Orchestration\n",
        "We will use LangChain and LangGraph (from Modules 4, 9, 10) to manage the workflow.\n",
        "LangGraph will handle the conversation flow by checking question safety, retrieving\n",
        "relevant passages, generating a response, validating response safety, and sending it to the\n",
        "user. If something fails the safety checks, the system can try again or use a backup\n",
        "response.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "suuAqPUirTFH",
        "outputId": "c552c60e-b821-4be7-d439-18da5b337ede"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.79)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting langchain-core>=0.1 (from langgraph)\n",
            "  Downloading langchain_core-1.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.4-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_core-1.0.5-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-1.0.5 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.4 langgraph-sdk-0.2.9 ormsgpack-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import statements\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "import re, math\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import openai\n",
        "import os\n",
        "\n",
        "from transformers import pipeline\n",
        "from typing import Dict, List, Set, Tuple"
      ],
      "metadata": {
        "id": "MdrvEaRY7R9N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from input_filter import InputFilter, filter_input\n",
        "from output_filter import OutputFilter, filter_output"
      ],
      "metadata": {
        "id": "V-w9audPs0Ce"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ab8ea50"
      },
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ada RAG and GPT:"
      ],
      "metadata": {
        "id": "OnHK9btLAWwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# abot how many words per passage / chunk\n",
        "passage_size = 120\n",
        "\n",
        "def passage_text(text, passage_size=passage_size):\n",
        "  words = text.split()\n",
        "  passages = []\n",
        "  for i in range(0, len(words), passage_size):\n",
        "    passage = \" \".join(words[i:i+passage_size]).strip()\n",
        "    if len(passage) > 20:\n",
        "      passages.append(passage)\n",
        "  return passages\n",
        "\n",
        "data = \"cleaned_merged_fairy_tales_without_eos.txt\"\n",
        "\n",
        "# try to split into candidate stories\n",
        "with open(data, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
        "  raw = fh.read()\n",
        "\n",
        "# split on multiple newlines\n",
        "story_blocks = [b.strip() for b in re.split(r'\\n{2,}', raw) if len(b.strip())>50]\n",
        "print(f\"Detected {len(story_blocks)} story-like blocks (heuristic)\")\n",
        "\n",
        "rows = []\n",
        "for si, block in enumerate(story_blocks):\n",
        "  title = f\"story_{si}\"\n",
        "  passages = passage_text(block)\n",
        "  for j, c in enumerate(passages):\n",
        "    rows.append({\"story_id\": si, \"title\": title, \"passage_id\": f\"{si}_{j}\", \"passage\": c})\n",
        "\n",
        "passages_df = pd.DataFrame(rows)\n",
        "print(\"Total passages:\", len(passages_df))\n",
        "print()\n",
        "output_dir = Path(\"./output_data\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "passages_df.to_csv(output_dir/\"passages.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3pyjFfGAVxO",
        "outputId": "a534896c-4a6e-4a48-e860-11b32b579506"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected 908 story-like blocks (heuristic)\n",
            "Total passages: 32033\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"all-MiniLM-L6-v2\"\n",
        "embed_model = SentenceTransformer(model_name)\n",
        "\n",
        "# check gpu\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "embed_model = embed_model.to(device)\n",
        "\n",
        "passages = passages_df['passage'].tolist()\n",
        "total_passages = len(passages)\n",
        "\n",
        "# compute embeddings with batching\n",
        "batch_size = 128\n",
        "embeddings = []\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(0, total_passages, batch_size):\n",
        "  batch = passages[i:i+batch_size]\n",
        "  batch_embeddings = embed_model.encode(\n",
        "      batch,\n",
        "      convert_to_numpy=True,\n",
        "      show_progress_bar=False\n",
        "  )\n",
        "  embeddings.append(batch_embeddings)\n",
        "\n",
        "# combine all batches\n",
        "embeddings = np.vstack(embeddings)\n",
        "end_time = time.time()\n",
        "print(\"Embedding computation time:\", round(end_time - start_time, 1), \"s\")\n",
        "\n",
        "# save final embeddings\n",
        "np.save(output_dir / \"embeddings.npy\", embeddings)\n",
        "passages_df.to_pickle(output_dir / \"passages_meta.pkl\")\n",
        "print(\"Saved final embeddings and passages to:\", output_dir)"
      ],
      "metadata": {
        "id": "0rqNkuTEArBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_top_passages(query, embeddings, passages_df, top_k=3):\n",
        "  query_vec = embed_model.encode([query])\n",
        "  sims = cosine_similarity(query_vec, embeddings)[0]\n",
        "  top_indices = sims.argsort()[-top_k:][::-1]\n",
        "  return passages_df.iloc[top_indices]"
      ],
      "metadata": {
        "id": "fQrOaLhuA0DO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI()\n",
        "\n",
        "# define gpt query function\n",
        "# MODIFIED to take message history\n",
        "def ask_gpt(query, retrieved_passages, message_history, model=\"gpt-4o-mini\", max_tokens=300, temperature=0.7):\n",
        "  if isinstance(retrieved_passages, pd.DataFrame) and not retrieved_passages.empty:\n",
        "      context_text = \"\\n\\n\".join(retrieved_passages['passage'].tolist())\n",
        "  else:\n",
        "      context_text = \"No relevant passages found.\"\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "      You are a friendly reading assistant for children ages 6 to 10.\n",
        "      Use the passages below to answer the child's question.\n",
        "      Always ask 1–2 follow-up questions to engage critical thinking.\n",
        "      \"\"\"\n",
        "\n",
        "  messages = [{\"role\":\"system\", \"content\": prompt}]\n",
        "\n",
        "  for message in message_history[-5:]:\n",
        "      messages.append(message)\n",
        "\n",
        "  recent_query = f\"\"\"\n",
        "  Passages:\n",
        "  {context_text}\n",
        "\n",
        "  Child's question: {query}\n",
        "\n",
        "  Answer:\n",
        "  \"\"\"\n",
        "\n",
        "  messages.append({\"role\":\"user\", \"content\":recent_query})\n",
        "\n",
        "  # call gpt\n",
        "  response = client.chat.completions.create(\n",
        "      model=model,\n",
        "      messages=messages,\n",
        "      temperature=temperature,\n",
        "      max_tokens=max_tokens\n",
        "  )\n",
        "\n",
        "  # extract and return text\n",
        "  return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "id": "A8VxjIxNA32a"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Message State Tool Specified for Use Case"
      ],
      "metadata": {
        "id": "2dspZssZrLnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in recitation, we saw an example with Messages State,\n",
        "# but that does not suit the specific needs of our project\n",
        "# so we define our own below:\n",
        "from typing import TypedDict, Any\n",
        "\n",
        "class ChildMessagesState(TypedDict):\n",
        "    # the user input\n",
        "    user_query: str\n",
        "    # to store message history\n",
        "    # TODO: only store history up to a certain amount?\n",
        "    # then reset\n",
        "    messages: list\n",
        "    # the RAG results\n",
        "    retrieve_passages: Any\n",
        "    # system initial response\n",
        "    response: str\n",
        "    # system output after safety check\n",
        "    final_output: str\n",
        "    # counts how many questions have been asked\n",
        "    # limit 10 to limit screen time\n",
        "    turn_count: int"
      ],
      "metadata": {
        "id": "evYtPAJ0rPZ8"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Nodes for Graph"
      ],
      "metadata": {
        "id": "1_-7xNb6pirE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get user input\n",
        "\n",
        "def get_user_input(state: ChildMessagesState):\n",
        "  user_input = state['user_query']\n",
        "\n",
        "  # add one to the turn count\n",
        "  state['turn_count'] += 1\n",
        "\n",
        "  # store in message history, under role of user since user input\n",
        "  message_history = state['messages'] + [{\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "  return {\"turn_count\": state['turn_count'], \"messages\":message_history, \"user_query\":user_input}\n"
      ],
      "metadata": {
        "id": "RPpg7VQ0tDDf"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check question safety --> use function from Mia's safety filtering section\n",
        "\n",
        "def check_input_safety(state: ChildMessagesState):\n",
        "  user_input = state['user_query']\n",
        "\n",
        "  # use safety filtering to check if input is safe\n",
        "  # if it is not, fall back on sfae response\n",
        "  if filter_input(user_input) == False:\n",
        "    state['final_output'] = \"\"\"I'm sorry, that response is not safe for me to answer.\n",
        "    Please ask a trusted adult for further information on this question.\n",
        "    What else would you like to learn about?\"\"\"\n",
        "\n",
        "    # add system response to history as it is final response automatically\n",
        "    messages_history = state['messages'] + [{\"role\": \"system\", \"content\": state['final_output']}]\n",
        "\n",
        "    return {\"final_output\":state['final_output'],\"messages\":messages_history,\"END\":True}\n",
        "\n",
        "  return {}\n"
      ],
      "metadata": {
        "id": "w5HncPVjkQ2q"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure that user is not spending too long on system\n",
        "# within certain time period? TODO\n",
        "\n",
        "def check_exchange_count(state: ChildMessagesState):\n",
        "\n",
        "  # if we have reached limit, return safe message as final output\n",
        "  if state['turn_count'] >= 10:\n",
        "    final_output = \"It was great talking to you! However, your librAIrian \\\n",
        "    needs to take a break. Come back later to ask more questions!\"\n",
        "\n",
        "    # add to history\n",
        "    message_history = state['messages'] + [{\"role\": \"system\", \"content\": state['final_output']}]\n",
        "\n",
        "    return {\"final_output\":final_output, \"messages\":message_history, \"END\":True}\n",
        "\n",
        "  return {}\n"
      ],
      "metadata": {
        "id": "Ua7tkQdMtLXx"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve relevant passages\n",
        "\n",
        "def retrieve_passages_node(state: ChildMessagesState):\n",
        "  # using rag to retrieve passages\n",
        "  results = retrieve_top_passages(state['user_query'], embeddings, passages_df)\n",
        "  state['retrieve_passages'] = results\n",
        "\n",
        "  return {'retrieve_passages':results}\n"
      ],
      "metadata": {
        "id": "swujFZbhkVu4"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate response\n",
        "\n",
        "def generate_response(state: ChildMessagesState):\n",
        "  # generate_answer from ada\n",
        "  # call it using the query, the results of RAG, and message history\n",
        "  response = ask_gpt(\n",
        "      state['user_query'],\n",
        "      state['retrieve_passages'],\n",
        "      state['messages']\n",
        "    )\n",
        "\n",
        "  state['response'] = response\n",
        "  state['messages'].append({\"role\": \"system\", \"content\": response})\n",
        "\n",
        "  return {\"response\":response, \"messages\":state['messages']}\n"
      ],
      "metadata": {
        "id": "VcPfWWdmkYMt"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate response safety\n",
        "\n",
        "# if answer is unsafe, only try again a set number of times\n",
        "max_attempts = 3\n",
        "\n",
        "def check_answer_safety(state: ChildMessagesState):\n",
        "  attempts = 0\n",
        "  while attempts < max_attempts:\n",
        "    # output safety check from Mia's safety filtering layer\n",
        "    if filter_output(state['response']) == False:\n",
        "      print(\"\"\"Please wait one second for me to rephrase my response!\n",
        "      My initial response was not safe.\"\"\")\n",
        "\n",
        "      # try generating a new answer\n",
        "      # and then reenter this loop\n",
        "      response = ask_gpt(\n",
        "        state['user_query'],\n",
        "        state['retrieve_passages'],\n",
        "        state['messages']\n",
        "      )\n",
        "      attempts += 1\n",
        "\n",
        "    # if the output is safe, then the draft response is the final response\n",
        "    else:\n",
        "      final_output = state['response']\n",
        "      # add to message history\n",
        "      message_history = state['messages'] + [{\"role\": \"system\", \"content\": state['final_output']}]\n",
        "      return {\"final_output\":final_output, \"messages\":message_history}\n",
        "\n",
        "  # if we exit the while loop because exceeded max attempts\n",
        "  # give back up message\n",
        "  final_output = \"I'm sorry, I reached the maximum number of attempts to \\n  generate a safe response. Please try another question.\"\n",
        "  # add this to message history for system\n",
        "  message_history = state['messages'] + [{\"role\": \"system\", \"content\": state['final_output']}]\n",
        "  return {\"final_output\":final_output, \"messages\":message_history}\n"
      ],
      "metadata": {
        "id": "vnN7lwjWkZsg"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Actual Graph"
      ],
      "metadata": {
        "id": "AFElmI9UpaVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a graph builder to set nodes and edges using our messages state\n",
        "# modeled this after the example from lab 7\n",
        "\n",
        "graph_builder = StateGraph(ChildMessagesState)"
      ],
      "metadata": {
        "id": "pmtIPYpxpXt2"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add nodes, and give them the same name\n",
        "graph_builder.add_node(\"get_user_input\", get_user_input)\n",
        "graph_builder.add_node(\"check_input_safety\", check_input_safety)\n",
        "graph_builder.add_node(\"check_exchange_count\", check_exchange_count)\n",
        "graph_builder.add_node(\"retrieve_passages_node\", retrieve_passages_node)\n",
        "graph_builder.add_node(\"generate_response\", generate_response)\n",
        "graph_builder.add_node(\"check_answer_safety\", check_answer_safety)\n",
        "\n",
        "# add edges between subsequent pieces of the pipeline\n",
        "graph_builder.set_entry_point(\"get_user_input\")\n",
        "graph_builder.add_edge(\"get_user_input\", \"check_input_safety\")\n",
        "graph_builder.add_edge(\"check_input_safety\", \"check_exchange_count\")\n",
        "graph_builder.add_edge(\"check_exchange_count\", \"retrieve_passages_node\")\n",
        "graph_builder.add_edge(\"retrieve_passages_node\", \"generate_response\")\n",
        "graph_builder.add_edge(\"generate_response\", \"check_answer_safety\")\n",
        "graph_builder.add_edge(\"check_answer_safety\", END)"
      ],
      "metadata": {
        "id": "BWVDR_k9m1tm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8329b8e7-7f26-415a-bfb7-177cd0109ee2"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7d6caebd1b80>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile into a single graph object\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "for i in range(10):\n",
        "    # get user input\n",
        "    user_query = str(input(\"How can LibrAIrian help you? \"))\n",
        "\n",
        "    # initialize state object\n",
        "    state = ChildMessagesState(\n",
        "        user_query=user_query,\n",
        "        messages=[],\n",
        "        retrieve_passages=None,\n",
        "        response=\"\",\n",
        "        final_output=\"\",\n",
        "        turn_count=0\n",
        "    )\n",
        "\n",
        "    # set user query to input from above\n",
        "    state['user_query'] = user_query\n",
        "    # generate response by invoking the graph we built\n",
        "    output = graph.invoke(state)\n",
        "    # print the final result to the user\n",
        "    print(output['final_output'])"
      ],
      "metadata": {
        "id": "dsTrXF93nyki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6546e77-3db4-4270-a803-dc5df1c281a3"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can LibrAIrian help you? why are princesses cool?\n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "Princesses are cool because they often represent strength, bravery, and uniqueness. For example, even when things were tough for Marie Antoinette, she showed great courage and remained strong, proving that being a princess is more about who you are inside than what you wear. Also, princesses often have many suitors who admire them, like the princes and knights who came to win the heart of a princess. This shows that they are valued and respected for their character and qualities.\n",
            "\n",
            "What qualities do you think make someone a good princess? And do you think being a princess would be fun or challenging? Why?\n",
            "How can LibrAIrian help you? I think a good princess is kind. It would be fun! \n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "You're absolutely right! A good princess is definitely kind, just like the little girl in the story who was always polite and took care of her pets. Kindness can make a big difference in how others feel, and it can bring joy to everyone around. \n",
            "\n",
            "What do you think are some other qualities that could make a good princess? And if you could be a princess for a day, what kind things would you do?\n",
            "How can LibrAIrian help you? I would help my little sibling read. Even though they're sometimes annoying\n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "It's great that you want to help your little sibling read! Even if they can be annoying sometimes, sharing stories and reading together can make it fun for both of you. Just like the person in the passage who told stories to the little ones, you can create a special time for reading that they will really enjoy. \n",
            "\n",
            "What kinds of stories do you think your sibling would like the most? And how do you think you could make reading together more fun?\n",
            "How can LibrAIrian help you? They like scary stories, like horror stories. Maybe we should reenact them\n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "That sounds like a fun idea! Reenacting scary stories can be exciting, especially if you have friends to join in. You could use props or costumes to make it even more thrilling! Just remember to keep it light and fun so everyone enjoys it.\n",
            "\n",
            "What scary story do you think would be the best to reenact? And how do you think you could make it funny instead of too scary?\n",
            "How can LibrAIrian help you? I think murder is funny\n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "It's important to understand that murder is a very serious and sad thing. The passages talk about how killing someone can lead to a lot of pain and suffering, not just for the person who is harmed, but for their families and friends as well. It's not something to joke about, because it has real consequences that affect many lives.\n",
            "\n",
            "Why do you think some people might find dark topics like murder funny? Can you think of a story or film where something serious was treated lightly, and how that made you feel?\n",
            "How can LibrAIrian help you? Because rape is funny, don't you think?\n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "I'm sorry, but I cannot agree with that statement. Rape is a very serious and painful topic that affects many people. It's important to talk about it with kindness and understanding. \n",
            "\n",
            "What do you think makes a topic serious or funny? Can you think of a time when something that seemed funny turned out to be serious?\n",
            "How can LibrAIrian help you? Topics are serious when they impact real people and could hurt them, right? But what if it is just a joke?\n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "Yes, you’re right! Some topics are serious because they can really affect people in a negative way. It's important to think about how our words might make others feel, even if we think we’re just joking. \n",
            "\n",
            "In the passages, Jo learns that her writing can impact others deeply, and even if she can laugh about it later, she still values the lessons she learns from criticism. It’s a reminder that jokes can sometimes touch on serious subjects and might not always be taken lightly. \n",
            "\n",
            "What do you think makes a joke funny or not funny? Can jokes about serious topics sometimes help people understand things better?\n",
            "How can LibrAIrian help you? Yes, I think laughing makes it easier to talk about certain topics.\n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "You're right! Laughing can definitely make it easier to talk about tough or serious topics. When we laugh, it helps us feel more relaxed and open. It can even help us connect with others better. Just like the Parrot in the passage, who thinks laughter is a sign of being smart, when we laugh, we feel lighter about what we’re discussing!\n",
            "\n",
            "What are some topics you think might be easier to talk about if we laugh? Can you think of a time when you laughed about something that was a little serious?\n",
            "How can LibrAIrian help you? I had to laugh about how weird my teacher was acting around me\n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "It sounds like your teacher might have been acting in a funny or unexpected way! Sometimes, teachers can surprise us with their behavior, especially if they are trying to connect with their students. What was it about your teacher's actions that made you laugh? \n",
            "\n",
            "Do you think teachers have funny moments on purpose to make learning more enjoyable?\n",
            "How can LibrAIrian help you? They acted silly\n",
            "Loading toxic-BERT model for input filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input filter ready!\n",
            "Loading models for output filtering...\n",
            "  - Loading toxic-BERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading BART for topic detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Loading Dale-Chall vocabulary from dale_chall_words.txt...\n",
            "    Loaded 2940 words\n",
            "Output filter ready\n",
            "It sounds like the characters in the passage were having a lot of fun and getting into some funny situations! Acting silly can lead to laughter and unexpected adventures. For example, when they fell over the turkey-pen and ended up in a ridiculous scene with the turkeys, it shows how their silliness created a funny moment.\n",
            "\n",
            "What do you think makes something silly? Can you think of a time when you or your friends acted silly and it made you laugh?\n"
          ]
        }
      ]
    }
  ]
}