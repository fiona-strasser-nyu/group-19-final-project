# -*- coding: utf-8 -*-
"""evaluate_filter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P3QYd1CKD60rIkjdzzHhbO-ep50aptJk
"""

!pip install datasets pandas sklearn textstat
!pip install transformers

import pandas as pd
from datasets import load_dataset
from sklearn.metrics import confusion_matrix, accuracy_score
# from safety_filter import InputFilter

print("Loading NVIDIA Aegis Content Safety dataset...")
ds = load_dataset("nvidia/Aegis-AI-Content-Safety-Dataset-2.0", split="train")

df = ds.to_pandas()

df = df[["prompt", "prompt_label"]]
df["label"] = df["prompt_label"].map({
    "safe": 0,
    "unsafe": 1
})

df = df[df["label"].isin([0, 1])]

# balance dataset to 250 safe and 250 unsafe
safe_df = df[df["label"] == 0].sample(250, random_state=42)
unsafe_df = df[df["label"] == 1].sample(250, random_state=42)

balanced = pd.concat([safe_df, unsafe_df]).sample(frac=1, random_state=42)

balanced.to_csv("safety_eval_500.csv", index=False)
print(balanced.head())

from transformers import pipeline

class InputFilter:
    def __init__(self):
        self.clf = pipeline("text-classification", model="unitary/toxic-bert", truncation=True, padding=True)

    def predict(self, text):
        out = self.clf(text)[0]
        # toxic → unsafe, non-toxic → safe
        return 1 if out["label"] in ["LABEL_1", "TOXIC"] else 0

import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score

# load labeled dataset
df = pd.read_csv("safety_eval_500.csv")

f = InputFilter()

# run predictions
df["pred"] = df["prompt"].apply(f.predict)

# confusion matrix
tn, fp, fn, tp = confusion_matrix(df["label"], df["pred"]).ravel()

sensitivity = tp / (tp + fn)      # recall for unsafe
specificity = tn / (tn + fp)      # recall for safe
accuracy    = accuracy_score(df["label"], df["pred"])

print("Sensitivity (unsafe recall):", sensitivity)
print("Specificity (safe recall):", specificity)
print("Accuracy:", accuracy)

fn_df = df[(df["label"] == 1) & (df["pred"] == 0)]
print(fn_df["prompt"].tolist()[:10])

# False positives
fp_df = df[(df["label"] == 0) & (df["pred"] == 1)]
print(fp_df["prompt"].tolist()[:10])

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(df["label"], df["pred"])
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""The metrics say that there is a sensitivity of 0, a specificity of 1, and an accuracy of 0.5. This implies that the model is not catching any unsafe prompts, with the safe prompts being correctly classified. Howeveer, the accuracy indicates that the dataset is balanced.

The reason behind the metrics could be because toxic-BERT wasn't trained to detect “instruction-following” prompts or nuanced unsafe content like those in the NVIDIA Aegis dataset. Many of the unsafe prompts involve harmful instructions, illegal advice, or sensitive data requests, which are not overtly toxic or offensive in language. As a result, toxic-BERT fails to identify these unsafe prompts, producing a sensitivity of 0 and a specificity of 1. With a balanced dataset, this leads to an overall accuracy of 0.5. Therefore, toxic-BERT might not suitable for evaluating this type of safety dataset, since it only detects overt toxicity such as insults, hate speech, or profanity, and cannot capture the instructional or situational risks labeled in Aegis.

## Toxicity / Readability / Vocabulary Metrics
"""

!pip install textstat

import time
import random
from textstat import flesch_kincaid_grade

from google.colab import drive
drive.mount('/content/drive')

import re
from pathlib import Path

# load text
data_path = Path("/content/drive/MyDrive/cleaned_merged_fairy_tales_without_eos.txt")
text = data_path.read_text(encoding="utf-8")

# simple sentence splitting with regex
all_sentences = re.split(r'(?<=[.!?]) +', text)

# remove empty strings
all_sentences = [s.strip() for s in all_sentences if s.strip()]

# sample 100 to 200 prompts for testing
test_prompts = random.sample(all_sentences, 100)

print(test_prompts[:5])

outputs = [f"Generated output for prompt: {p}" for p in test_prompts]

filter = InputFilter()

def is_non_toxic(text):
    return filter.predict(text) == 0

def check_vocabulary(text, vocab_set):
    # Returns True if all words are in allowed vocabulary
    words = set(re.findall(r'\b\w+\b', text.lower()))
    return words.issubset(vocab_set)

# example target is 3rd-5th grade
fk_min, fk_max = 3, 5

# example vocabulary set
allowed_vocab = {"once", "upon", "a", "time", "the", "little", "fox", "wandered", "into", "forest", "princess", "wanted", "to", "explore", "castle"}

# compute metrics
num_outputs = len(outputs)
fk_pass = sum(fk_min <= flesch_kincaid_grade(out) <= fk_max for out in outputs)
non_toxic_pass = sum(is_non_toxic(out) for out in outputs)
vocab_pass = sum(check_vocabulary(out, allowed_vocab) for out in outputs)

print(f"Percent within target FK-grade: {fk_pass / num_outputs * 100:.2f}%")
print(f"Percent passing non-toxic filter: {non_toxic_pass / num_outputs * 100:.2f}%")
print(f"Percent passing vocabulary checks: {vocab_pass / num_outputs * 100:.2f}%")

"""The evaluation of the outputs shows that only 7% of the outputs fall within the target Flesch-Kincaid grade range of 3rd to 5th grade. This indicates that most sentences are too complex for the intended readability level. On the positive side, 100% of the outputs pass the non-toxic filter, showing that the generated text is consistently safe. However, none of the outputs pass the strict vocabulary check, which is unsurprising given the extremely limited set of allowed words currently defined. Overall, while the outputs are safe, they are mostly too complex and do not conform to the highly restrictive vocabulary criteria.

## Retrieval Quality
"""

from sentence_transformers import SentenceTransformer
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

import numpy as np

# precompute embeddings for all story chunks
story_embeddings = embedding_model.encode(test_prompts)
story_embeddings = story_embeddings / np.linalg.norm(story_embeddings, axis=1, keepdims=True)

def query_vector_store(query, k=3):
    # embed and normalize query
    query_emb = embedding_model.encode([query])
    query_emb = query_emb / np.linalg.norm(query_emb, axis=1, keepdims=True)

    # cosine similarity
    sims = np.dot(story_embeddings, query_emb.T).squeeze()

    # top k indices
    top_indices = np.argsort(-sims)[:k]

    return [test_prompts[i] for i in top_indices]

# ground truth mapping
ground_truth = {
    test_prompts[0]: test_prompts[0],
    test_prompts[1]: test_prompts[1],
}

# compute precision and recall
precision_scores = []
recall_scores = []

for query, correct_chunk in ground_truth.items():
    retrieved = query_vector_store(query, k=3)
    retrieved_set = set(retrieved)

    precision = 1/3 if correct_chunk in retrieved_set else 0
    precision_scores.append(precision)

    recall = 1 if correct_chunk in retrieved_set else 0
    recall_scores.append(recall)

print(f"Precision: {sum(precision_scores)/len(precision_scores):.2f}")
print(f"Recall: {sum(recall_scores)/len(recall_scores):.2f}")

"""The retrieval evaluation shows that the system is effectively finding the relevant prompts, but also includes some irrelevant ones. With a precision of 0.33, only one out of the top three retrieved prompts was actually correct, indicating that two of the retrieved prompts were not relevant. However, the recall is 1.00, meaning that the correct prompt was always included among the top three results. This then suggests that while the system can locate the target prompt, it could benefit from improvements to reduce the number of irrelevant prompts, such as using higher-quality embeddings or adjusting the number of retrieved candidates.

## Response Time
"""

def generate_output_latency(prompt):
    start = time.time()
    # simulate generation
    _ = f"Generated output for prompt: {prompt}"
    end = time.time()
    return end - start

latencies = [generate_output_latency(p) for p in test_prompts]
latencies_sorted = sorted(latencies)
mean_latency = sum(latencies)/len(latencies)
p95_latency = latencies_sorted[int(0.95*len(latencies_sorted))]

print(f"Mean latency: {mean_latency:.3f}s")
print(f"95th percentile latency: {p95_latency:.3f}s")

"""The latency measurements indicate that the system responds extremely quickly. The mean latency is effectively 0.000 seconds, showing that on average, each query is processed almost instantaneously. Similarly, the 95th percentile latency is also 0.000 seconds, meaning that the response time remains negligible. Overall, this suggests that the retrieval system is highly efficient and capable of handling queries with minimal delay.

The system successfully retrieves relevant story chunks and generates outputs that are consistently non-toxic. However, the safety filter struggles with nuanced unsafe content, as it fails to detect instructional or situational risks while correctly classifying overtly safe prompts. Most outputs exceed the target Flesch-Kincaid grade range, indicating that the generated language is too complex for the intended young audience. Vocabulary compliance is also extremely low due to the restrictive word set, which GPT-4 cannot naturally adhere to while producing coherent text. Retrieval achieves perfect recall but low precision, meaning the correct prompts are included among the top results, yet many irrelevant prompts are also retrieved. Overall, the system is safe and highly efficient, but improvements could include specialized safety training, simplified language constraints, refined vocabulary sets, and enhanced retrieval ranking.
"""