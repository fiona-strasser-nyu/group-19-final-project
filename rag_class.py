# -*- coding: utf-8 -*-
"""rag_class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qzRqyNOluTP2iY0HHfAiV4lF5q-Is3vf
"""

import re, math
import pandas as pd
from sentence_transformers import SentenceTransformer
import numpy as np
import torch
import time
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
import openai

class RAG_GPT:
  def __init__(self, file_name):
    self.file_name = file_name
    self.device = None
    self.total_passages = 0
    self.embeddings_shape = None
    self.embedding_time = 0
    self.embed_model = None
    self.output_dir = Path("./output_data")

  def passage_text(self, text, passage_size):
    words = text.split()
    passages = []
    for i in range(0, len(words), passage_size):
      passage = " ".join(words[i:i+passage_size]).strip()
      if len(passage) > 20:
        passages.append(passage)
    return passages

  def process_data(self):
    data = self.file_name

    with open(data, "r", encoding="utf-8") as f:
      for _ in range(10):
        print(f.readline().strip())

    # abot how many words per passage / chunk
    passage_size = 120

     # try to split into candidate stories
    with open(data, "r", encoding="utf-8", errors="replace") as fh:
      raw = fh.read()

    # split on multiple newlines
    story_blocks = [b.strip() for b in re.split(r'\n{2,}', raw) if len(b.strip())>50]

    rows = []
    for si, block in enumerate(story_blocks):
      title = f"story_{si}"
      passages = self.passage_text(block, passage_size)
      for j, c in enumerate(passages):
        rows.append({"story_id": si, "title": title, "passage_id": f"{si}_{j}", "passage": c})

    passages_df = pd.DataFrame(rows)

    output_dir = Path("/content/drive/MyDrive/librarian_project")
    output_dir.mkdir(parents=True, exist_ok=True)
    passages_df.to_csv(output_dir/"passages.csv", index=False)

    return passages_df

  def build_embeddings(self, passages_df):
      model_name = "all-MiniLM-L6-v2"
      embed_model = SentenceTransformer(model_name)
      self.embed_model = embed_model

      # check gpu
      device = "cuda" if torch.cuda.is_available() else "cpu"
      self.device = device
      embed_model = embed_model.to(device)

      passages = passages_df['passage'].tolist()
      total_passages = len(passages)
      self.total_passages = total_passages

      # compute embeddings with batching
      batch_size = 128
      embeddings = []

      start_time = time.time()
      for i in range(0, total_passages, batch_size):
        batch = passages[i:i+batch_size]
        batch_embeddings = embed_model.encode(
            batch,
            convert_to_numpy=True,
            show_progress_bar=False
        )
        embeddings.append(batch_embeddings)

      # combine all batches
      embeddings = np.vstack(embeddings)
      end_time = time.time()
      self.embeddings_shape = embeddings.shape
      self.embedding_time = end_time - start_time

      # save final embeddings
      # instead of Path in Drive as it is in demo, create a doc in the same directory
      output_dir = Path("./output_data")
      output_dir.mkdir(parents=True, exist_ok=True)
      np.save(output_dir / "embeddings.npy", embeddings)
      passages_df.to_pickle(output_dir / "passages_meta.pkl")

      return output_dir

  def retrieve_top_passages(query, embeddings, passages_df, top_k=3):
      query_vec = embed_model.encode([query])
      sims = cosine_similarity(query_vec, embeddings)[0]
      top_indices = sims.argsort()[-top_k:][::-1]
      return passages_df.iloc[top_indices]

  # define gpt query function
  def ask_gpt(query, retrieved_passages, model="gpt-4o-mini", max_tokens=300, temperature=0.7):
      context_text = "\n\n".join(retrieved_passages['passage'].tolist())

      prompt = f"""
  You are a friendly reading assistant for children ages 6 to 10.
  Use the passages below to answer the child's question.
  Always ask 1â€“2 follow-up questions to engage critical thinking.

  Passages:
  {context_text}

  Child's question: {query}

  Answer:
  """
          # call gpt
      response = openai.chat.completions.create(
          model=model,
          messages=[{"role": "user", "content": prompt}],
          temperature=temperature,
          max_tokens=max_tokens
      )

      # extract and return text
      return response.choices[0].message.content.strip()

def create_embeddings(file_name):
  embeddings_info = RAG_GPT(file_name)

  passages_df = embeddings_info.process_data()
  output_directory = embeddings_info.build_embeddings(passages_df)

  return output_directory

def rag_passages(user_query, output_directory):
  embeddings = np.load(output_directory / "embeddings.npy")
  passages_df = pd.read_pickle(output_directory / "passages_meta.pkl")

  retrieve_top_passages(user_query, embeddings, passages_df, top_k=3)

  return passages

def generate_response(query, top_passages):
  response = ask_gpt(query, top_passages, model="gpt-4o-mini")

  return response